# app/api/chat.py
import os
import json
import logging
from typing import List, Literal, Optional, Tuple
import uuid

from fastapi import APIRouter, Header
from pydantic import BaseModel, Field
from openai import OpenAI, AuthenticationError, APIConnectionError, APIStatusError

from app.core.db import SessionLocal
from app import models  # MemoryItem, Bundle ë“±

logger = logging.getLogger("app.chat")

# ----- FastAPI Router -----
router = APIRouter()

# ----- OpenAI ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜ -----
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or ""
SHARED_API_PASSWORD = os.getenv("SHARED_API_PASSWORD") or ""

if OPENAI_API_KEY:
  logger.info("[chat.py] OPENAI_API_KEY is set (server shared key ê°€ëŠ¥).")
else:
  logger.warning("[chat.py] OPENAI_API_KEY is not set. Server shared key ì‚¬ìš© ë¶ˆê°€.")

if SHARED_API_PASSWORD:
  logger.info("[chat.py] SHARED_API_PASSWORD is set (í‰ê°€ìš© ëª¨ë“œ).")
else:
  logger.warning("[chat.py] SHARED_API_PASSWORD is not set. shared password ê¸°ëŠ¥ ë¹„í™œì„±í™”.")

# í•œ ë²ˆì— LLMì— ë³´ë‚¼ ìµœëŒ€ íˆìŠ¤í† ë¦¬ ê¸¸ì´
MAX_HISTORY = 10
# memory_contextë¡œ ë¶™ì¼ ìµœëŒ€ ë©”ëª¨ ê°œìˆ˜
MAX_MEMORY_ITEMS = 8


# ----- Pydantic ëª¨ë¸ -----
class ChatHistoryItem(BaseModel):
    role: Literal["user", "assistant"]
    content: str


class ChatRequest(BaseModel):
    user_id: uuid.UUID
    message: str
    history: List[ChatHistoryItem] = Field(default_factory=list)
    # ê¸°ì¡´: ë²ˆë“¤ ì„ íƒ
    selected_bundle_ids: List[uuid.UUID] = Field(default_factory=list)
    # ìƒˆ í•„ë“œ: í”„ë¡ íŠ¸ì—ì„œ ì²´í¬í•œ ë©”ëª¨ idë“¤
    selected_memory_ids: List[uuid.UUID] = Field(default_factory=list)


class UsedMemoryItem(BaseModel):
    id: str
    bundle_id: str
    title: Optional[str] = None


class ChatResponse(BaseModel):
    # í”„ë¡ íŠ¸ì—ì„œ ê¸°ëŒ€í•˜ëŠ” í•„ë“œ ì´ë¦„ì— ë§ì¶°ì¤Œ (ì§€ê¸ˆì€ answer ì‚¬ìš© ì¤‘)
    answer: str
    memory_context: str = ""
    used_memories: List[UsedMemoryItem] = Field(default_factory=list)
# =========================
#  OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± í—¬í¼
# =========================
def build_openai_client(
    user_api_key: Optional[str],
    shared_api_password: Optional[str],
) -> Optional[OpenAI]:
    """
    ìš°ì„ ìˆœìœ„:
    1) user_api_key (ê°œì¸ í‚¤)
    2) shared_api_password == SHARED_API_PASSWORD ì¸ ê²½ìš° ì„œë²„ OPENAI_API_KEY
    ë‘˜ ë‹¤ ì—†ìœ¼ë©´ None (echo ëª¨ë“œ)
    """
    # 1) ì‚¬ìš©ì ê°œì¸ í‚¤
    if user_api_key:
        try:
            return OpenAI(api_key=user_api_key)
        except Exception as e:
            logger.warning("[chat.py] invalid user OpenAI key: %r", e)

    # 2) í‰ê°€ìš© ë¹„ë°€ë²ˆí˜¸ â†’ ì„œë²„ ê³µìš© í‚¤ ì‚¬ìš©
    if (
        shared_api_password
        and SHARED_API_PASSWORD
        and shared_api_password == SHARED_API_PASSWORD
        and OPENAI_API_KEY
    ):
        try:
            logger.info("[chat.py] using SERVER shared OPENAI_API_KEY via password.")
            return OpenAI(api_key=OPENAI_API_KEY)
        except Exception as e:
            logger.warning("[chat.py] failed to build shared OpenAI client: %r", e)

    # 3) ë‘˜ ë‹¤ ì‹¤íŒ¨ â†’ None
    return None


# =========================
#  helper: memory_context
# =========================
def build_memory_context(
    user_id: uuid.UUID,
    bundle_ids: Optional[List[uuid.UUID]],
    selected_memory_ids: Optional[List[uuid.UUID]],
) -> Tuple[str, List[UsedMemoryItem]]:
    """
    selected_memory_idsê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ê·¸ ë©”ëª¨ë“¤ë§Œ ì‚¬ìš©.
    ë¹„ì–´ ìˆìœ¼ë©´ bundle_ids ê¸°ì¤€ìœ¼ë¡œ ê¸°ì¡´ ë™ì‘ ìœ ì§€.
    """
    db = SessionLocal()
    try:
        q = db.query(models.MemoryItem).filter(models.MemoryItem.user_id == user_id)

        if selected_memory_ids:
            # âœ… ì²´í¬í•œ ë©”ëª¨ë§Œ ì‚¬ìš©
            q = q.filter(models.MemoryItem.id.in_(selected_memory_ids))
        elif bundle_ids:
            # ì˜ˆì „ ë°©ì‹: ë²ˆë“¤ ì „ì²´
            q = q.filter(models.MemoryItem.bundle_id.in_(bundle_ids))
        else:
            # ì•„ë¬´ê²ƒë„ ì„ íƒ ì•ˆ í–ˆìœ¼ë©´ memory_context ì—†ìŒ
            return "", []

        q = q.order_by(models.MemoryItem.created_at.desc()).limit(MAX_MEMORY_ITEMS)
        rows = q.all()

        if not rows:
            return "", []

        lines: List[str] = []
        used: List[UsedMemoryItem] = []

        for m in rows:
            title = getattr(m, "title", None)
            summary = getattr(m, "summary", None)
            original_text = getattr(m, "original_text", "")

            text_for_context = summary or original_text or ""
            if len(text_for_context) > 200:
                text_for_context = text_for_context[:200] + "..."

            if title:
                line = f"- ({title}) {text_for_context}"
            else:
                line = f"- {text_for_context}"

            lines.append(line)

            used.append(
                UsedMemoryItem(
                    id=str(m.id),
                    bundle_id=str(m.bundle_id),
                    title=title,
                )
            )

        context_text = "\n".join(lines)
        return context_text, used

    except Exception as e:
        logger.exception("[chat.py] build_memory_context error: %r", e)
        return "", []
    finally:
        db.close()


# =========================
#  helper: ìš”ì•½ + í‚¤ì›Œë“œ
# =========================
def summarize_and_extract_keywords(
    user_message: str,
    answer: str,
    client: Optional[OpenAI],
) -> Tuple[str, List[str]]:
    """
    ì±„íŒ… 1í„´(ì‚¬ìš©ì ë©”ì‹œì§€ + LLM ë‹µë³€)ì„ ìš”ì•½í•˜ê³  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë½‘ëŠ”ë‹¤.
    ë„ˆë¬´ ì •êµí•˜ì§€ ì•Šì•„ë„ ë˜ëŠ” 1ì°¨ ë²„ì „.
    """
    # API KEY ì—†ìœ¼ë©´ ëŒ€ì¶© ì§§ê²Œ ìë¥´ê¸°
    if client is None:
        combined = f"ì‚¬ìš©ì: {user_message}\n\nLLM: {answer}"
        summary = combined[:200]
        return summary, []
    ...
    # ë‚˜ë¨¸ì§€ ë¡œì§ì€ ê·¸ëŒ€ë¡œ, client ì‚¬ìš©
    try:
        resp = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•˜ê³  í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=256,
            temperature=0.2,
        )

        content = resp.choices[0].message.content or ""
        data = json.loads(content)

        summary = str(data.get("summary", "")).strip()
        keywords_raw = data.get("keywords", []) or []
        keywords = [str(k).strip() for k in keywords_raw if str(k).strip()]

        # ì•ˆì „ì¥ì¹˜
        if not summary:
            summary = (f"ì‚¬ìš©ì: {user_message}\nLLM: {answer}")[:200]

        return summary, keywords
    except Exception as e:
        logger.warning(
            "[chat.py] summarize_and_extract_keywords failed: %r", e
        )
        combined = f"ì‚¬ìš©ì: {user_message}\n\nLLM: {answer}"
        return combined[:200], []


# =========================
#  helper: ë²ˆë“¤ ì„ íƒ/ìƒì„±
# =========================
def pick_or_create_bundle_for_chat(
    user_id: uuid.UUID,
    summary: str,
    keywords: List[str],
) -> models.Bundle:
    """
    1) ìœ ì €ì˜ ë²ˆë“¤ë“¤ ì¤‘ì—ì„œ í‚¤ì›Œë“œì™€ ê°€ì¥ ì˜ ë§ëŠ” ë²ˆë“¤ ê³ ë¦„
    2) ì—†ìœ¼ë©´ ìƒˆ ë²ˆë“¤ ìƒì„±
    (ê°„ë‹¨ ë¬¸ìì—´ ë§¤ì¹­ ë²„ì „)
    """
    db = SessionLocal()
    try:
        bundles: List[models.Bundle] = (
            db.query(models.Bundle)
            .filter(models.Bundle.user_id == user_id, models.Bundle.is_archived == False)  # noqa: E712
            .all()
        )

        # ë²ˆë“¤ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë¬´ì¡°ê±´ ìƒˆë¡œ ìƒì„±
        def _make_new_bundle() -> models.Bundle:
            base_name = ""
            if keywords:
                base_name = keywords[0]
            if not base_name:
                base_name = summary[:20] or "ìë™ ìƒì„± ë²ˆë“¤"

            new_bundle = models.Bundle(
                user_id=user_id,
                name=base_name,
                description="ìë™ ìƒì„± (ìš”ì•½/í‚¤ì›Œë“œ ê¸°ë°˜)",
                color="#4F46E5",
                icon="ğŸ“",
            )
            db.add(new_bundle)
            db.commit()
            db.refresh(new_bundle)
            return new_bundle

        if not bundles:
            return _make_new_bundle()

        lower_keywords = [k.lower() for k in keywords if k]
        best_bundle: Optional[models.Bundle] = None
        best_score = 0

        for b in bundles:
            text = ((b.name or "") + " " + (b.description or "")).lower()
            score = 0
            for kw in lower_keywords:
                if kw and kw in text:
                    score += 1

            if score > best_score:
                best_score = score
                best_bundle = b

        # ì ìˆ˜ê°€ 0ì´ë©´ "ê´€ë ¨ ë²ˆë“¤ ì—†ìŒ"ìœ¼ë¡œ ë³´ê³  ìƒˆë¡œ ìƒì„±
        if best_bundle is None or best_score == 0:
            return _make_new_bundle()

        return best_bundle

    finally:
        db.close()


# =========================
#  helper: ìë™ ë¶„ë¥˜+ì €ì¥
# =========================
def auto_route_and_save_chat_memory(
    user_id: uuid.UUID,
    user_message: str,
    llm_answer: str,
    client: Optional[OpenAI],
) -> Optional[models.MemoryItem]:
    """
    1) ìš”ì•½ + í‚¤ì›Œë“œ ì¶”ì¶œ
    2) ì ì ˆí•œ ë²ˆë“¤ ì„ íƒ ë˜ëŠ” ìƒˆ ë²ˆë“¤ ìƒì„±
    3) í•´ë‹¹ ë²ˆë“¤ì— MemoryItem ìƒì„±

    ì‹¤íŒ¨í•´ë„ ì „ì²´ /chat íë¦„ì€ ê¹¨ì§€ì§€ ì•Šë„ë¡ ì˜ˆì™¸ëŠ” ìœ„ë¡œ ì•ˆ ì˜¬ë¦¼.
    """
    db = SessionLocal()
    try:
        summary, keywords = summarize_and_extract_keywords(user_message, llm_answer, client)
        bundle = pick_or_create_bundle_for_chat(user_id, summary, keywords)

        original_text = f"ì‚¬ìš©ì: {user_message}\n\nLLM: {llm_answer}"

        # ---- ì œëª©ì„ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ê¸° ----
        if keywords:
            # í‚¤ì›Œë“œ 3~4ê°œ ì •ë„ë¥¼ "/"ë¡œ ì´ì–´ ë¶™ì´ê¸°
            title = " / ".join(keywords[:4])
        elif summary:
            title = summary[:50]
        else:
            title = "ìë™ ìš”ì•½ ë©”ëª¨"

        mem = models.MemoryItem(
            user_id=user_id,
            bundle_id=bundle.id,
            title=title,
            original_text=original_text,
            summary=summary,
            source_type="auto_chat",  # í•„ìš”ì‹œ enums ë§ê²Œ ìˆ˜ì •
            source_id=None,
            metadata_json={
                "auto_routed": True,
                "keywords": keywords,
            },
        )
        db.add(mem)
        db.commit()
        db.refresh(mem)

        logger.info(
            "[auto_route] saved memory id=%s into bundle id=%s (name=%s)",
            mem.id,
            bundle.id,
            bundle.name,
        )
        return mem
    except Exception as e:
        logger.exception("[chat.py] auto_route_and_save_chat_memory error: %r", e)
        return None
    finally:
        db.close()


# =========================
#         /chat
# =========================
@router.post("/chat", response_model=ChatResponse)
async def chat_endpoint(
    req: ChatRequest,
    x_openai_key: Optional[str] = Header(None),
    x_shared_api_password: Optional[str] = Header(None),
) -> ChatResponse:
    logger.info(
        "[CHAT REQUEST] user_id=%s message=%r history_len=%d "
        "selected_bundle_ids=%s selected_memory_ids=%s",
        req.user_id,
        req.message,
        len(req.history),
        req.selected_bundle_ids,
        req.selected_memory_ids,
    )

    # ìš”ì²­ë§ˆë‹¤ ì ì ˆí•œ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = build_openai_client(x_openai_key, x_shared_api_password)

    # history ìŠ¬ë¼ì´ì‹±
    if len(req.history) > MAX_HISTORY:
        history_for_llm = req.history[-MAX_HISTORY:]
    else:
        history_for_llm = req.history

    logger.info(
        "[CHAT] using history_len=%d (original=%d)",
        len(history_for_llm),
        len(req.history),
    )

    # memory_context êµ¬ì„± (ì²´í¬ëœ ë©”ëª¨ ê¸°ë°˜)
    memory_context_text, used_memories = build_memory_context(
        user_id=req.user_id,
        bundle_ids=req.selected_bundle_ids,
        selected_memory_ids=req.selected_memory_ids,
    )

    # system í”„ë¡¬í”„íŠ¸
    base_system_prompt = (
        "You are an assistant that helps the user with their projects. "
        "Answer in Korean by default unless the user uses another language."
    )

    if memory_context_text:
        system_content = (
            f"{base_system_prompt}\n\n"
            "[memory_context]\n"
            f"{memory_context_text}\n"
            "[/memory_context]"
        )
    else:
        system_content = base_system_prompt

    messages = [{"role": "system", "content": system_content}]
    for h in history_for_llm:
        messages.append({"role": h.role, "content": h.content})
    messages.append({"role": "user", "content": req.message})

    # ï¿½ OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìœ¼ë©´ echo ëª¨ë“œ
    if client is None:
        logger.warning("[chat.py] No OpenAI client for this request. Echo mode.")
        reply_text = f"[NO_API_KEY] echo: {req.message}"
        return ChatResponse(
            answer=reply_text,
            memory_context=memory_context_text,
            used_memories=used_memories,
        )

    # ë””ë²„ê·¸ìš© payload ë¡œê·¸ (ë‚´ìš©ì€ ê·¸ëŒ€ë¡œ)
    try:
        logger.info(
            "[LLM REQUEST PAYLOAD]\n%s",
            json.dumps(
                {
                  "model": "gpt-4.1-mini",
                  "messages": messages,
                  "max_tokens": 512,
                  "temperature": 0.7,
                },
                ensure_ascii=False,
                indent=2,
            ),
        )
    except Exception:
        pass

    try:
        completion = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=messages,
            max_tokens=512,
            temperature=0.7,
        )
        reply_text = completion.choices[0].message.content or ""
        logger.info("[LLM RESPONSE] %r", reply_text)

        # âœ… ìë™ ë¶„ë¥˜ + ì €ì¥ (ë™ì¼ client ì‚¬ìš©)
        try:
            auto_route_and_save_chat_memory(
                user_id=req.user_id,
                user_message=req.message,
                llm_answer=reply_text,
                client=client,
            )
        except Exception as e:
            logger.warning("[chat.py] auto_route failed (ignored): %r", e)

        return ChatResponse(
            answer=reply_text,
            memory_context=memory_context_text,
            used_memories=used_memories,
        )


    except AuthenticationError as e:
        logger.warning("[chat.py] AuthenticationError: %r", e)
        return ChatResponse(
            answer=f"[AUTH_ERROR] API í‚¤ ì¸ì¦ ì˜¤ë¥˜ë¡œ echo ëª¨ë“œë¡œ ì‘ë‹µí•©ë‹ˆë‹¤: {req.message}",
            memory_context=memory_context_text,
            used_memories=used_memories,
        )
    except APIConnectionError as e:
        logger.warning("[chat.py] APIConnectionError: %r", e)
        return ChatResponse(
            answer=f"[NETWORK_ERROR] OpenAI ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ì–´ echo ëª¨ë“œë¡œ ì‘ë‹µí•©ë‹ˆë‹¤: {req.message}",
            memory_context=memory_context_text,
            used_memories=used_memories,
        )
    except APIStatusError as e:
        logger.warning("[chat.py] APIStatusError: %r", e)
        return ChatResponse(
            answer=f"[OPENAI_STATUS_ERROR] ìƒíƒœì½”ë“œ={e.status_code}, echo: {req.message}",
            memory_context=memory_context_text,
            used_memories=used_memories,
        )
    except Exception as e:
        logger.exception("[chat.py] UNKNOWN ERROR: %r", e)
        return ChatResponse(
            answer=f"[UNKNOWN_ERROR] ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ë¡œ echo ëª¨ë“œë¡œ ì‘ë‹µí•©ë‹ˆë‹¤: {req.message}",
            memory_context=memory_context_text,
            used_memories=used_memories,
        )
